# Chess bot (Beaver)

Beaver is a chess engine that generates the best move for a given position. It uses deep neural network for estimating the heuristics of a certain position,
and alpha-beta pruning for searching through the game tree.

Neural network is initially trained using supervised learning, based on preexisting dataset of positions and their evaluations, after which it is retrained by reinforcement learning (using temporal difference).

At a search depth of only 5 moves ahead, engine plays at around 1700ELO rating.

## About the engine

### Dataset
- You can download the dataset [here](https://storage.googleapis.com/chesspic/datasets/2021-07-31-lichess-evaluations-37MM.db.gz). It consists of about 37 milion chess positions (gathered from games played on site [lichess.org](https://lichess.org/)) and evaluation for each position. Each evaluation was generated by Stockfish engine, with evaluations ranging from -152 to 152. 

### Data preprocess
- Positions from the dataset are represented using FEN notation. In order to train the network, each position is loaded and converted into an array of 773 elements, which is a board representation that is much more suitable for neural networks. <br/>
Said array consists of the following information: 
  - each square is represented by an array of 12 elements created by one-hot-encoding a figure (there are 6 types of figures in chess, each one can be either white or black). It requires 6x2x64=768 elements in total.
  - one element represents which side is to play next (1 if white plays next, -1 if black plays next)
  - one element represents whether white has short castling rights (1 if it does, 0 if it doesn't)
  - one element represents whether white has long castling rights (1 if it does, 0 if it doesn't)
  - one element represents whether black has short castling rights (-1 if it does, 0 if it doesn't)
  - one element represents whether black has short castling rights (-1 if it does, 0 if it doesn't)
- As for evaluations, even though they range from -152 to 152, most of them contain values which are significantly lower. Since normalizing or standardizing values in smaller ranges will cause many of them to converge to 0, additional transformations are required beforehand. 
```math
  f(x) =\begin{cases}-4.3 - log(-x) & x <-5\\x & -5 \leq  x  \leq 5\\4.3 + log(x) & x > 5\end{cases} 
```
  - By doing so, smaller values of evaluations are relatively preserved, while the larger values are scaled down. Since positions with evaluations larger than 5 (or smaller than -5) are usually evaluations of winning positions for either side, the exact values of predictions of those positions are not that important. Doing these preprocessing transformations before normalization has increased the speed of training, as well as precision of neural network predictions.

### Neural network
- Neural network consists of three layers, which consist of 4096, 2048 and 2048 neurons.
- Each layer has ReLU activation function. In addition to this, every layer has dropout with 0.4 ratio which is added for regularization.
- Output layer which consists of 1 neuron, and it has linear activation function as its output.
- After training, model is quantized using [TensorFlow Lite](https://www.tensorflow.org/lite/performance/post_training_quantization) in order to reduce model inference time.

### Alpha-beta pruning
- Alpha-beta pruning is used to find the next best move based on the current position. In this implementation, a couple of improvements have been added to this alghoritm in order to reduce search time:
  - the alghoritm sorts the positions at the first move (based on evaluations given by neural network), in order to search through best evaluated positions first, which would speed up search if those positions are indeed winning for the player (and would slow down the search if they are not winning vice versa)
  - transposition tables are added (each position is memorised along with its evaluation and best found move, in case the position is repeated during search)
  - iterative deepening is added (alghoritm first searches positions at search depth which is lower then desired by 3, so it can fill transposition table before the full search)
  - positions that are searched through first within each node are those that include capturing, castling or giving check. This reduces the time of the search since these moves have higher chances of being the optimal moves in given positions.
- MDT(f) variation of alpha-beta pruning has also been tested, however it did not prove better than beforementioned alpha-beta pruning with augmentations.

### Reinforcement learning
- Chess engine was further trained using reinforcement learning. The method that was used is named temporal difference learning. Although the engine plays on a similar level as the original one (trained without RL), it has shown progress by having better predictions (evaluations) for certain positions. In order to improve the engine using this method, further training is required (the engine has to play with itself a huge number of games to show progress, which in turn requires a lot of physical resources).

### Results
- The engine was tested by playing with Stockfish engine, which was set on different levels of play (based on ELO system).
- The engine beats Stockfish set at 1700ELO in about 6 out of 10 games on average.
- It has also been tested on [Arasan Test Suite](https://www.arasanchess.org/testsuite.shtml) for which it found about 15% of best moves for given positions, which rates it among engines that have advanced, semiprofessional level of play.

## Using the application
### Requirements
- All the dependencies listed in the <em>requirements.txt</em> file have to be installed, preferably inside an enviroment created with venv specifically for this programme. 
- To install the requirements enter the following command in your terminal:
```console
pip install -r requirements.txt
``` 
### Run
- In order to run the agent, you need to pass a path to the model as an argument.
```console
python run_agent.py --model_path "model-path"
```
- In order to train the network, you need to pass the train and validation dataset files' paths (both files have to be in ".csv" format) as arguments. Results will be logged by MlFlow.
```console
python neural_network.py --train_dataset_path "train-dataset-path" --val_dataset_path "val-dataset-path"
```
- In order to train the network using reinforcement learning, you need to pass the train and validation dataset files' paths (both files have to be in ".csv" format), as well as a path to the model (which is to be trained) as arguments. Results will be logged by MlFlow.
```console
python reinforcement_learning.py --model_path "model-path" --train_dataset_path "train-dataset-path" --val_dataset_path "val-dataset-path"
```

### Runing the agent
- In order to run the agent, you will first need to unzip the contents of one of the files located at <em>beaver_model</em> (unzip <em>beaver_nn.zip</em if you want to use supervised version of the agent, or <em>beaver_rl.zip</em> in case if you want to use the agent trained using RL), in which neural network's model and weights are saved. 
- After you start the agent, you will be asked to input position in FEN notation, for which the agent will find the best move.

### Output
- After the application is done with searching for the best move, you should get an output similar to this: <br><br>
 ![output](https://user-images.githubusercontent.com/58399701/165513086-d4316f77-05c8-420a-bbfe-12fa8f8be4ec.png)
- Move heuristics represents the value of position after making the recommended move (closer to 1 is winning for white, closer to -1 is winning for black, while 0 is drawn position).
- Move represents a recommended move in UCI format.
- Solve time represents number of seconds in which the application has found the recommended move.
